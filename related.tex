Empirical use of language features papers. Rust unsafe.

We discuss related work in the areas of focal method detection,
% declarative versus imperative static analysis,
treatment of containers, and taint analysis.

\paragraph{Focal methods and classes} To situate Ghafari et al's work~\cite{ghafari15:_autom}, a number of previous works have studied test-to-code traceability by identifying focal \emph{classes} for a test case---the classes which are tested by a test case. The focal \emph{methods} we discuss in this paper belong to focal classes. Ghafari et al were the first to extend the study of traceability to focal methods. Before that, Qusef et al proposed techniques which identify focal classes. In~\cite{DBLP:conf/icsm/QusefBOLB11}, they propose a two-stage approach relying on the assumption that the last assertion statement in a test case is the key assertion. The first stage uses dynamic slicing to find all classes that contribute to the values tested in the assertion (possibly including mocks), while the second stage filters classes and keeps only those textually closest to the test class. An additional mock object filter would help remove definitely-not-focal classes. Earlier work by Qusef et al~\cite{DBLP:conf/icsm/QusefOL10} uses dataflow analysis instead of dynamic slicing.

Rompaey and Demeyer~\cite{rompaey09:_estab_traceab_links_unit_test} evaluate six other heuristics for finding focal classes: naming conventions, types referred to in tests (``fixture element types''), the static call graph, the last call before the assert, lexical analysis of the code and the test, and co-evolution of the test and the main code. No heuristic dominates: different heuristics work for different codebases. Our approach adds another way to rule out unwanted focal method and focal class results.

Ying and Tarr~\cite{DBLP:conf/eclipse/YingT07} also propose heuristics to filter out unwanted methods during code inspection. Their heuristics are based on characteristics of the call graph, i.e. they filter out small methods and methods closer to the bottom of a call graph, depending on tuneable parameters. These heuristics empirically eliminate mock calls in their benchmarks, but there is no principled reason for that to be the case, and indeed, the static call graph that they depend on should not interact well with mock calls.

\paragraph{Treatment of containers} In this work, we use coarse-grained abstractions for containers, consistent with the approach from Chu et al~\cite{chu12:_collec_disjoin_analy}. We do not observe sophisticated container manipulations where it would be necessary to track exactly which elements of a container are mocks. Were such an analysis necessary, the fine-grained container client analysis by Dillig et al~\cite{dillig11:_precis_reason_progr_using_contain} would work.

\paragraph{Taint analysis} Like many other static analyses, our mock analysis can be seen as a variant of a static taint analysis: sources are mock creation methods, while sinks are method invocations. There are no sanitizers in our case. However, for a taint analysis, there is usually a small set of sink methods, while in our case, every method invocation in a test method is a potential sink. In some ways, our analysis resembles an information flow analysis like that by Clark et al~\cite{clark07:_static_analy_quant_infor_flow}. However, the goal of our analysis (detecting possible mocks) is different from taint and information flow analyses in that it is not security-sensitive, so the balance between false positives and false negatives is different---it is less critical to not miss any potential important mock invocations, whereas missing a whole class of tainted methods would often be unacceptable.


%% \paragraph{Imperative vs declarative}
%% Kildall contributed perhaps the first dataflow analysis~\cite{kildall73:_unified_approac_global_progr_optim} as the concept is understood today, describing an algorithm for intraprocedural constant propagation and common subexpression elimination. His algorithm, operating on the program graph, is described in quite imperative pseudocode (and proven to terminate). In some sense, implementing algorithms imperatively is the default, and doesn't need further discussion, except to point out that program analysis frameworks such as Soot~\cite{Vallee-Rai:1999:SJB:781995.782008} provide libraries that can ease the implementation burden.

%% To our knowledge, Corsini et al did some of the first work in declarative program analysis~\cite{corsini93:_effic}; however, that work performed abstract interpretation on (tiny) logic programs rather than imperative programs. Dawson et al~\cite{dawson96:_pract_progr_analy_using_gener} did similar work. Around the same time, Reps proposed~\cite{Reps1995} a declarative analysis to perform demand versions of interprocedural program analyses, which is similar to what we have here; however, we compute all of the analysis results rather than performing a demand analysis. CodeQuest by Hajijev et al~\cite{hajiyev06} also allows developers to perform AST-level code queries using a declarative query language. {\sc Dimple$^+$}~\cite{benton07:_inter_scalab_declar_progr_analy}\cite[Chapter 3]{benton08:_fast_effec_progr_analy_objec_level_paral} by Benton and Fischer may be closest to what we are advocating as the declarative analysis approach. While Benton's dissertation presents a simple {\sc Dimple$^+$} implementation of Andersen's points-to analysis, the {\sc Dimple$^+$} work does not have Doop's sophisticated pointer analysis available to it. Soufflé, by Scholz et al~\cite{scholz16:_fast_large_scale_progr_analy_datal}, advocates for declarative static analysis (but without comparing it directly to an imperative approach as we do here), and presents performance optimizations needed to achieve this goal.
%% Finally, Doop~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}, which is now primarily implemented with a Soufflé backend, is perhaps the most powerful extant declarative program analysis, and focusses on expressing sophisticated pointer analyses in Datalog. 



%% % implementation note: Reps's approach is much more complicated than what we have in Doop. Perhaps Doop's use of SSA and simulation of phi nodes allows it to use much simpler rules, or maybe it's the specific analyses that are being implemented. e.g. for Doop, which computes an overapproximation, merging the two branches using the virtual phi node (simulated as "x = phi(x1,x2) => x = x1; x = x2") works just fine.

%% In terms of comparing implementations, Prakash et al~\cite{prakash21:_effec_progr_repres_point_analy} compare pointer analysis as provided by Doop and Wala; in some sense, the present work is similar to that work in that both works compare two frameworks. However, that work compares empirical results from two families of pointer analysis implementations (and finds that the specific intermediate representation used doesn't change the results much), while we discuss the process of implementing a static analysis declaratively versus imperatively. Like us, they note that Doop is difficult to incorporate into a program transformation framework (it works better in standalone mode) while Wala's results are readily available; a similar result applies to any result that a Soot-based data flow analysis produces as compared to a Doop-based declarative analysis.


